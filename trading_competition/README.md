# Trading AI Competition: RL vs Genetic Algorithms

Sistema de competiciÃ³n entre agentes de Reinforcement Learning y Algoritmos GenÃ©ticos para trading algorÃ­tmico.

## ğŸ“Š Resultados de la CompeticiÃ³n

| Agente | Valor Final | Retorno Total | NÃºmero de Trades |
|--------|-------------|---------------|------------------|
| **GA** | **$12,057.13** | **20.57%** | 24 |
| RL | $10,000.00 | 0.00% | 0 |

ğŸ† **Ganador: Agente GA** (margen: 20.57%)

## ğŸ—ï¸ Arquitectura del Proyecto

```
trading_competition/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ rl_agent.py          # Agente RL con PPO
â”‚   â””â”€â”€ ga_agent.py          # Agente GA con DEAP
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Datos crudos de SPY
â”‚   â””â”€â”€ processed/           # Datos con indicadores tÃ©cnicos
â”œâ”€â”€ environments/
â”‚   â””â”€â”€ trading_env.py       # Entorno Gymnasium personalizado
â”œâ”€â”€ models/                  # Modelos entrenados guardados
â”œâ”€â”€ results/                 # Resultados y visualizaciones
â”œâ”€â”€ strategies/              # Estrategias adicionales
â”œâ”€â”€ tests/                   # Tests del sistema
â””â”€â”€ utils/                   # Utilidades
```

## ğŸš€ Componentes Implementados

### âœ… Completado
- **Entorno de Desarrollo**: Python 3.11.9 con venv y 15+ paquetes
- **AdquisiciÃ³n de Datos**: 1458 dÃ­as de datos SPY (2020-2025)
- **Indicadores TÃ©cnicos**: 30+ indicadores (RSI, MACD, Bollinger Bands, etc.)
- **Agente RL**: PPO con Stable-Baselines3, entrenado 10k timesteps
- **Agente GA**: EvoluciÃ³n genÃ©tica con DEAP, fitness 89.4%
- **Framework de CompeticiÃ³n**: ComparaciÃ³n automÃ¡tica en datos de prueba
- **Visualizaciones**: GrÃ¡ficos comparativos de rendimiento

### ğŸ”„ Pendiente
- **Backtesting Avanzado**: IntegraciÃ³n con backtesting.py
- **OptimizaciÃ³n RL**: Mejorar funciÃ³n de recompensa y entrenamiento

## ğŸ§  Agentes Desarrollados

### Agente RL (PPO)
- **Framework**: Stable-Baselines3
- **Algoritmo**: Proximal Policy Optimization
- **Estado**: 11 indicadores tÃ©cnicos normalizados
- **Acciones**: Hold/Buy/Sell
- **Recompensa**: Retorno del portfolio + penalizaciones
- **Resultado**: PolÃ­tica conservadora (0 trades en competiciÃ³n)

### Agente GA (DEAP)
- **Framework**: DEAP
- **Cromosoma**: [RSI_overbought, RSI_oversold, MACD_threshold, BB_width]
- **Fitness**: Retorno total del portfolio
- **EvoluciÃ³n**: 30 generaciones, poblaciÃ³n 50
- **Resultado**: 20.57% retorno, 24 trades optimizados

## ğŸ“ˆ Indicadores TÃ©cnicos

- **Momentum**: RSI, Stochastic, MACD, Williams %R
- **Tendencia**: SMA, EMA, ADX, DMP/DMN
- **Volatilidad**: ATR, Bollinger Bands, Volatility 20d
- **Volumen**: OBV, CMF, Volume
- **Retornos**: Returns 1d/5d/20d, Log returns

## ğŸ› ï¸ InstalaciÃ³n y Uso

```bash
# Clonar y configurar entorno
git clone <repo>
cd trading_competition
python -m venv .venv
.venv\Scripts\activate  # Windows
pip install -r requirements.txt

# Ejecutar pipeline completo
python create_structure.py
python download_data.py
python indicators.py
python agents/rl_agent.py
python agents/ga_agent.py
python competition.py
```

## ğŸ¯ Conclusiones

1. **GA SuperÃ³ a RL**: El agente genÃ©tico encontrÃ³ una estrategia superior (20.57% vs 0%)
2. **Interpretabilidad**: GA proporciona parÃ¡metros claros vs "caja negra" del RL
3. **Eficiencia**: GA converge mÃ¡s rÃ¡pido que RL en este dominio
4. **Limitaciones RL**: FunciÃ³n de recompensa necesita refinamiento

## ğŸ”¬ PrÃ³ximos Pasos

- Mejorar reward function del RL (Sharpe ratio, drawdown)
- Implementar ensemble de agentes
- Agregar mÃ¡s indicadores y features
- ValidaciÃ³n walk-forward y out-of-sample
- IntegraciÃ³n con brokers reales (Paper trading)

## ğŸ“š TecnologÃ­as Utilizadas

- **Python 3.11.9**
- **Stable-Baselines3** (RL)
- **DEAP** (GA)
- **Gymnasium** (Entornos RL)
- **TA-Lib** (Indicadores tÃ©cnicos)
- **Pandas/NumPy** (Procesamiento de datos)
- **Matplotlib/Seaborn** (Visualizaciones)
- **Rich** (CLI mejorada)